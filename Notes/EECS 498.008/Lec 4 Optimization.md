# 如何找到$argmin_w(L(w))$
### 随机搜索
### 梯度下降
**随机梯度下降法** - 每次用一个样本来进行梯度下降
	问题：可能陷入局部最优以及鞍点 ; 数据存在噪声会让梯度下降方向不准确 
#### **SGD + momentum (随机下降 + 惯性)**
![[Pasted image 20250305165533.png|500]]
#### **Momentum**
保留上一个梯度的部分信息
![[Pasted image 20250305165939.png|350]]
#### **Neserov Momentum** 
计算沿速度因子行走一段时间后的梯度 从而叠加出actual step
![[Pasted image 20250305170740.png|400]]![[Pasted image 20250305170800.png|300]]
#### **Adagrad**
自适应学习率优化算法 
设置学习率 $$\alpha = \frac{\alpha_0}{\sqrt{s+\epsilon}}$$
 其中s为每次计算梯度平方的累加 ，$\epsilon$ 为一个小量，防止分母为零
 **缺点**： 可能会提早停止训练

 #### **RMSProp** 
	$$g = \nabla_{\theta_{k-1}} L(\theta)$$
	$$r_k = \beta r_{k-1} + (1-\beta) g \odot g$$
	$$\eta = \frac{\eta}{\sqrt{r_k + \epsilon}}$$
	$$\theta_k = \theta_{k-1} - \eta g$$
	$g$ 是损失函数关于参数的梯度
	$r_k$​ 是累积平方梯度的移动平均
	$\beta$ 是移动平均的衰减率
    $\odot$ 表示逐元素相乘
	$\epsilon$ 是一个小常数，用于数值稳定性
	 $\eta$ 是学习率
	$\theta_k$ 是第k次迭代后的参数值
    **缺点** : 依赖超参数的选择；可能不适合所有的问题
 #### **Adam(RMSProp + Momentum))**
 1. 梯度计算：
$$g = \nabla_{\theta_{k-1}} L(\theta)$$
- $g$：当前梯度
- $θ$：模型参数
- $L(θ)$：损失函数
- $∇$：梯度算子

2. 一阶矩估计：
$$m_k = \beta_1 m_{k-1} + (1-\beta_1)g$$

- $m_k$：一阶矩估计（梯度的指数移动平均）
- $β_1$：一阶矩衰减率（通常设为0.9）
- $m_{k-1}$：前一时刻的一阶矩估计

3. 二阶矩估计：
$$v_k = \beta_2 v_{k-1} + (1-\beta_2)g \odot g$$

- $v_k$：二阶矩估计（梯度平方的指数移动平均）
- $β_2$：二阶矩衰减率（通常设为0.999）
- $v_{k-1}$：前一时刻的二阶矩估计
- $⊙$：逐元素相乘（使用 $\odot$ 表示）

4. 一阶矩偏差修正：
$$\hat{m}_k = \frac{m_k}{1-\beta_1^k}$$
- $\hat{m}_k$：偏差修正后的一阶矩估计
- $k$：当前迭代次数

5. 二阶矩偏差修正：
$$\hat{v}_k = \frac{v_k}{1-\beta_2^k}$$
- $\hat{v}_k$：偏差修正后的二阶矩估计

6. 参数更新：

$$\theta_k = \theta_{k-1} - \frac{\eta}{\sqrt{\hat{v}_k} + \epsilon}\hat{m}_k$$
- $θ_k$：更新后的参数
- $θ_{k-1}$：当前参数
- $η$：学习率（通常设为0.001）
- $ε$：小常数，防止除零（通常设为1e-8）
Adam优化器的主要特点：
1. 结合了 Momentum 和 RMSprop 的优点
2. 自适应学习率，不同参数有不同的更新幅度
3. 通过偏差修正解决训练初期估计不准确的问题
4. 在深度学习中广泛使用，适用于大多数非凸优化问题

常用超参数默认值:

- $\beta_1 = 0.9$
- $\beta_2 = 0.999$
- $\eta = 0.001$
- $\epsilon = 10^{-8}$


![[Pasted image 20250305172920.png]]

 - 在许多情况下，Adam 是一个很好的默认选择
- SGD+Momentum 可以胜过 Adam，但可能需要更多调整