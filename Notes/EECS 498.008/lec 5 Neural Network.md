### Image Features
#### Histogram of Oriented Gradients(方向梯度直方图)

#### Bag of Words(Data_Driven)
### 神经网络可以计算任何函数的原因(Universality theorems 万能近似定理)
#### 声明—注意事项
- 万能近似定理并不意味着神经网络可以用来计算任何函数。相反，我们可以得到一个近似值，这就符合我们的要求了。（增加隐藏神经元的数量可以得到更好的近似值）
- 可以近似的函数只能是连续函数。（并不重要）
#### Bump 函数的构建
神经网络通过一层隐藏层并将参数设置为指定比例，则可以得到表现较为优异的Bump函数。并且可以多设置几个隐藏层中的单元，可以产生多个指向不同的Bump函数。
![[Pasted image 20250305201301.png]]
使用多个bump函数，就可以近似等效为目标函数
#### Tower 函数的构建
原理近似于Bump函数的构建，即保持特征参数为指定比例
![[Pasted image 20250305201644.png]]
以此类推，多维的环境也可以构建像以上两种的单位函数，从而构建出整个想要近似的函数

### Neural Networks
![[Pasted image 20250305191545.png|475]]
	max() 为一个激活函数 ReLU(z) = max(0, z)
#### Activation Functions
![[Pasted image 20250305192442.png|600]]
 
**神经网络的类比解释:**
	神经元有输入和输出，在输入接收信息，并在神经元内部有一个处理器（非线性函数）对收集到的信息进行总结和处理，最后输出到下一个神经元。
	一些区别：神经元构成的网络十分复杂，可能有回环等 但是在神经网络中一般都是分层

**神经网络层与层之间的解释：**
![[Pasted image 20250305193846.png]]
W矩阵可以看作是坐标轴的线性变换；而激活函数可以在一定程度上促进特征的提取，从而让数据能够线性可分
神经网络层数越多，非线性划分就会越多。（层数越多可能会出现过拟合的情况,如下图）
![[Pasted image 20250305194136.png]]
	**不要**为了解决过拟合的问题而降低层数，应当加入正则化参数来弱化高次项的作用
**神经网络的收敛性（place of magic）**
    凸函数 ：![[Pasted image 20250305203255.png]]
     凸平面同理。
     神经网络研究的是非凸优化的问题。这一问题没有收敛性的证明，但是却可以得到收敛的值。